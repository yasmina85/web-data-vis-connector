{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Web APIs\n",
    "\n",
    "A growing number of organizations make data sets available on the web in a style called REST, which stands for REpresentational State Transfer. When REST is used, every data set is identified by a URL and can be accessed through a set of functions called an Application Programming Interface (API). \n",
    "\n",
    "**Topics Covered:**\n",
    "- [Getting data from the Web](#Getting data from the Web)\n",
    "- Examples\n",
    "    * [Climate Data API](#Climate Data API)\n",
    "    * [NYtimes API](#NYtimes API)\n",
    "    * [Twitter API](#twitterapi)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**References:**\n",
    "* [Working With Data on the Web](http://swcarpentry.github.io/web-data-python/01-getdata/)\n",
    "* [Accessing Databases via Web APIs](https://github.com/Data-on-the-Mind/2017-summer-workshop/blob/master/hench-data-from-web/01-APIs/01-API_workbook.ipynb)\n",
    "* [Data and Twitter](https://github.com/henchc/EDUC290B/blob/master/02-Data-and-Twitter.ipynb)\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from the Web\n",
    "\n",
    "### How do GET Requests Work?  A Web browsing example\n",
    "\n",
    "* Surfing the Web = Making a bunch of GET Requests\n",
    "\n",
    "* For instance, I open my web browser and type in http://www.wikipedia.org.  Once I hit return, I'd see a webpage\n",
    "\n",
    "* Several different processes occured, however, between me hitting \"return\" and the page finally being rendered\n",
    "\n",
    "\n",
    "\n",
    "### Step 1: The GET Request\n",
    "\n",
    "* web browser took the entered character string \n",
    "* used the command-line tool \"Curl\" to write a properly formatted HTTP GET request \n",
    "* submitted it to the server that hosts the Wikipedia homepage\n",
    "\n",
    "---\n",
    "### STEP 2: The Response\n",
    "\n",
    "* Wikipedia's server receives this request\n",
    "* send back an HTTP response\n",
    "* from which Curl extracted the HTML code for the page\n",
    "\n",
    "```{html}\n",
    "[1] \"<!DOCTYPE html>\\n<html lang=\\\"mul\\\" dir=\\\"ltr\\\">\\n<head>\\n<!-- Sysops: Please do not edit the main template directly; update /temp and synchronise. -->\\n<meta charset=\\\"utf-8\\\">\\n<title>Wikipedia</title>\\n<!--[if lt IE 7]><meta http-equiv=\\\"imagetoolbar\\\" content=\\\"no\\\"><![endif]-->\\n<meta name=\\\"viewport\\\" content=\\\"i\"\n",
    "```\n",
    "\n",
    "---\n",
    "### STEP 3: The Formatting\n",
    "\n",
    "* raw HTML code was formatted and executed by the web browser\n",
    "* rendering the page as seen in the window.\n",
    "\n",
    "---\n",
    "\n",
    "### Web Browsing as a Template for RESTful Database Querying\n",
    "\n",
    "The process of web browsing described above is a close analogue for the process of database querying via RESTful APIs, with only a few adjustments:\n",
    "\n",
    "1. While the Curl tool will still be used to send HTML GET requests to the servers hosting our databases of interest, the character string that we supply to Curl must be constructed so that the resulting request can be interpreted and succesfully acted upon by the server.  In particular, it is likely that the character string must encode **search terms and/or filtering parameters**, as well as one or more **authentication codes**.  While the terms are often similar across APIs, most are API-specific.\n",
    "\n",
    "2. Unlike with web browsing, the content of the server's response that is extracted by Curl is unlikely to be HTML code.  Rather, it will likely be **raw text response that can be parsed into one of a few file formats commonly used for data storage**.  The usual suspects include .csv, .xml, and .json files.\n",
    "\n",
    "3. Whereas the web browser capably parsed and executed the HTML code, **one or more facilities in R, Python, or other programming languages will be necessary for parsing the server response and converting it into a format for local storage** (e.g. matrices, dataframes, databases, lists, etc.).\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, run this command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Climate Data API\n",
    "The Climate Data API provides programmatic access to most of the climate data used on the World Bank’s [Climate Change Knowledge Portal](http://sdwebx.worldbank.org/climateportal/). Check out the World Bank’s [Terms of Use](https://data.worldbank.org/summary-terms-of-use). According to the API’s home page, the data sets containing yearly averages for various values are identified by URLs of the form:\n",
    "\n",
    "http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/var/year/iso3.ext\n",
    "\n",
    "where:\n",
    "\n",
    "* var is either pr (for precipitation) or tas (for “temperature at surface”);\n",
    "* iso3 is the International Standards Organization (ISO) 3-letter code for a country, such as “CAN” for Canada or “BRA” for Brazil; and\n",
    "* ext (short for “extension”) specifies the format we want the data in. There are several choices for format, but the simplest is comma-separated values (CSV), in which each record is a row, and the values in each row are separated by commas. (CSV is frequently used for spreadsheet data.)\n",
    "\n",
    "For example, if we want the average annual temperature in Canada as a CSV file, the URL is:\n",
    "\n",
    "http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/CAN.csv\n",
    "\n",
    "If we paste that URL into a browser, it displays:\n",
    "~~~\n",
    "year,data\n",
    "1901,-7.67241907119751\n",
    "1902,-7.862711429595947\n",
    "1903,-7.910782814025879\n",
    "...\n",
    "2007,-6.819293975830078\n",
    "2008,-7.2008957862854\n",
    "2009,-6.997011661529541\n",
    "~~~\n",
    "\n",
    "This particular data set might be stored in a file on the World Bank’s server, or that server might:\n",
    "\n",
    "1. Receive our URL.\n",
    "2. Break it into pieces.\n",
    "3. Extract the three key fields (the variable, the country code, and the desired format).\n",
    "4. Fetch the desired data from a database.\n",
    "5. Format the data as CSV.\n",
    "6. Send that to our browser.\n",
    "\n",
    "As long as the World Bank doesn’t change its URLs, we don’t need to know which method it’s using and it can switch back and forth between them without breaking our programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports the requests library\n",
    "import requests\n",
    "#defines the URL for the data we want; \n",
    "#we could just pass this URL as an argument to the requests.get \n",
    "url = 'http://climatedataapi.worldbank.org/climateweb/rest/v1/country/cru/tas/year/CAN.csv'\n",
    "#Initiate GET request and assign the response to an object\n",
    "response = requests.get(url)\n",
    "#\n",
    "if response.status_code != 200:\n",
    "    print('Failed to get data:', response.status_code)\n",
    "else:\n",
    "    print('First 100 characters of data are')\n",
    "    print(response.text[:100]) #Assign the data sent back by the web server to the object’s text member variable and print it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### NYTimes API\n",
    "\n",
    "How Much Less Popular is Duke Ellington?\n",
    "\n",
    "If you ask a jazz musician who they feel is the greatest bandleader of all time, there's a pretty good chance they'll mention Duke Ellington. Though Ellington was at peak popularity from roughly 1930 to 1945, his music is still heard regularly.\n",
    "\n",
    "TASK: Characterize the popularity of Duke Ellington over the past 15 years. Specifically, is he \"trending\"?\n",
    "\n",
    "### STEP 1: Finding Data Resources\n",
    "\n",
    "To determine the popularity of something, we need a measurement of how frequently or widely it is referenced or encountered.  Moreover, to determine how this popularity changes over time, we need a measurement that is taken repeatedly.\n",
    "\n",
    "Newspapers are an excellent source of such information.  The frequency with which certain items appear in its pages can be a decent metric of its popularity, and its continual publication creates a built-in time series.  And while there are a variety of newspapers to choose from, we'll be working with the New York Times for a variety of reasons --- including its status as a paper of record, its long publishing history, and (most importantly) its convenient article API.\n",
    "\n",
    "[NYT Article API](http://developer.nytimes.com/)\n",
    "\n",
    "### STEP 2: Getting API Access\n",
    "\n",
    "For most APIs, a key or other user credentials are required for any database querying.  Generally, this requires that you register with the organization.  Most APIs are set up for developers, so you'll likely be asked to register an \"application\".  All this really entails is coming up with a name for your app/bot/project, and providing your real name, organization, and email.  Note that some more popular APIs (e.g. Twitter, Facebook) will require additional information, such as a web address or mobile number.\n",
    "\n",
    "Once you've successfully registered, you will be assigned one or more keys, tokens, or other credentials that must be supplied to the server as part of any API call you make.  To make sure that users aren't abusing their data access privileges (e.g. by making many rapid queries), each set of keys will be given several **rate limits** governing the total number of calls that can be made over certain intervals of time.  For the NYT Article API, we have relatively generous rate limits --- 10 calls per second and 10,000 calls per day.\n",
    "\n",
    "[NYT Article API Keys](http://developer.nytimes.com/apps/mykeys)\n",
    "\n",
    "### STEP 3: Learning how to Construct API GET Requests\n",
    "\n",
    "Likely the most challenging part of using web APIs is learning how to format your GET request URLs.  While there are common architectures for such URLs, each API has its own unique quirks.  For this reason, carefully reviewing the API documentation is critical.\n",
    "\n",
    "Fortunately, the NYT Article API is [very well documented](http://developer.nytimes.com/docs/read/article_search_api_v2)!\n",
    "\n",
    "----\n",
    "Most GET request URLs for API querying have three or four components:\n",
    "\n",
    "1. *Base URL*: a link stub that will be at the beginning of all calls to a given API; points the server to the location of an entire database\n",
    "\n",
    "2. *Search Parameters*: a character string appended to a base URL that tells the server what to extract from the database; basically a series of filters used to point to specific parts of a database\n",
    "\n",
    "3. *Authenication Key/Token*: a user-specific character string appended to a base URL telling the server who is making the query; allows servers to efficiently manage database access\n",
    "\n",
    "4. *Response Format*: a character string indicating how the response should be formatted; usually one of .csv, .json, or .xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests  # to make the GET request \n",
    "import json  # to parse the JSON response to a Python dictionary\n",
    "import time  # to pause after each API call\n",
    "import csv  # to write our data to a CSV\n",
    "import pandas  # to see our CSV\n",
    "\n",
    "\n",
    "#Step 1: Construct GET request (a base URL for the API, some authorization code or key, and, a format for the response.)\n",
    "# set key. Use the following demonstration keys for now, but in the future, get your own\n",
    "key=\"be8992a420bfd16cf65e8757f77a5403:8:44644296\"\n",
    "\n",
    "# set base url\n",
    "base_url=\"http://api.nytimes.com/svc/search/v2/articlesearch\"\n",
    "\n",
    "# set response format\n",
    "response_format=\".json\"\n",
    "\n",
    "# set search parameters\n",
    "search_params = {\"q\":\"Duke Ellington\",\n",
    "                 \"api-key\":key}   \n",
    "r = requests.get(base_url+response_format, params=search_params) #response object called r\n",
    "\n",
    "#Uncomment the following line to see what it will print\n",
    "#print(r.url)\n",
    "#Click on the link to see what happens: \n",
    "#http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Duke+Ellington&api-key=be8992a420bfd16cf65e8757f77a5403%3A8%3A44644296\n",
    "\n",
    "\n",
    "\n",
    "# Inspect the content of the response, parsing the result as text\n",
    "response_text= r.text\n",
    "#Uncomment the following line to see what it will print\n",
    "#print(response_text[:1000])\n",
    "\n",
    "# Convert JSON response to a dictionary\n",
    "data = json.loads(response_text)\n",
    "#Uncomment the following line to see what it will print\n",
    "# data\n",
    "\n",
    "#Commands to work with json data\n",
    "\n",
    "#Print the status\n",
    "print(data['status'])\n",
    "\n",
    "#put the data in variable.\n",
    "docs = data['response']['docs']\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import time\n",
    "from random import randint\n",
    "import requests\n",
    "import json\n",
    "from __future__ import division\n",
    "import math\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# DEFINE YOUR FUNCTION HERE\n",
    "# set key\n",
    "key=\"be8992a420bfd16cf65e8757f77a5403:8:44644296\"\n",
    "\n",
    "def get_api_data(term, year):\n",
    "    # set base url\n",
    "    base_url=\"http://api.nytimes.com/svc/search/v2/articlesearch\"\n",
    "\n",
    "    # set response format\n",
    "    response_format=\".json\"\n",
    "\n",
    "    # set search parameters\n",
    "    search_params = {\"q\":term,\n",
    "                 \"api-key\":key,\n",
    "                 \"begin_date\": str(year) + \"0101\", # date must be in YYYYMMDD format\n",
    "                 \"end_date\":str(year) + \"1231\"}\n",
    "\n",
    "    # make request\n",
    "    r = requests.get(base_url+response_format, params=search_params)\n",
    "    \n",
    "    # convert to a dictionary\n",
    "    data=json.loads(r.text)\n",
    "    \n",
    "    # get number of hits\n",
    "    hits = data['response']['meta']['hits']\n",
    "    print(\"number of hits:\", str(hits))\n",
    "    \n",
    "    # get number of pages\n",
    "    pages = int(math.ceil(hits/10))\n",
    "    \n",
    "    # make an empty list where we'll hold all of our docs for every page\n",
    "    all_docs = [] \n",
    "    \n",
    "    # now we're ready to loop through the pages\n",
    "    for i in range(pages):\n",
    "        print(\"collecting page\", str(i))\n",
    "        \n",
    "        # set the page parameter\n",
    "        search_params['page'] = i\n",
    "        \n",
    "        # make request\n",
    "        r = requests.get(base_url+response_format, params=search_params)\n",
    "    \n",
    "        # get text and convert to a dictionary\n",
    "        data=json.loads(r.text)\n",
    "        \n",
    "        # get just the docs\n",
    "        docs = data['response']['docs']\n",
    "        \n",
    "        # add those docs to the big list\n",
    "        all_docs = all_docs + docs\n",
    "        \n",
    "        time.sleep(randint(3,5))  # pause between calls\n",
    "        \n",
    "    return(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_api_data(\"Duke Ellington\", 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Twitter API\n",
    "\n",
    "This [Twitter API](https://dev.twitter.com/overview/api) is slightly more complicated, but because of this, people have created very useful tools to easily interact with the Twitter API. First, follow the directions to get your API credientials.\n",
    "1. [Create a Twitter account](https://twitter.com).  You can use an existing account if you have one.\n",
    "2. Under account settings, add your phone number to the account.\n",
    "3. [Create a Twitter developer account](https://dev.twitter.com/resources/signup).  Attach it to your Twitter account.\n",
    "4. Once you're logged into your developer account, [create an application for this course](https://apps.twitter.com/app/new).  You can call it whatever you want, and you can write any URL when it asks for a web site.\n",
    "5. On the page for that application, find your Consumer Key and Consumer Secret.\n",
    "6. On the same page, create an Access Token.  Record the resulting Access Token and Access Token Secret.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "import tweepy  # This halps us access Twitter data.\n",
    "import matplotlib.pyplot as plt #This is for plotting \n",
    "\n",
    "# Twitter API credentials\n",
    "# Note that these credentials are for demonstration \n",
    "consumer_key = \"IjI8AdEUOlzif3J0qgt6bw9JI\"\n",
    "consumer_secret = \"gZLhygPv5uBCWIQVr6sZjCgYVfcXGzGuTNl7oOapYmWazdLEm6\"\n",
    "access_key = \"278661116-qdUru3GXVYT9upGH0cgbwROu4KzypMSwQgknMNW2\"\n",
    "access_secret = \"RMZY9H7vvbuHq9jFZO4fdtw5cBPZUlbLDhEwU9zir6LyG\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Establish a query using the word 'Berkeley'\n",
    "results = tweepy.Cursor(\n",
    "    api.search,\n",
    "    q='Berkeley', # query, any word you want found in a tweet\n",
    "    result_type = 'popular'\n",
    "    ).items(20)\n",
    "\n",
    "# define an empty list called results_tweets\n",
    "results_tweets = []\n",
    "\n",
    "# Iterate over the first tweets in `results` and add each of those tweets to results_tweets\n",
    "for t in results:\n",
    "    results_tweets.append(t) \n",
    "\n",
    "\n",
    "#print the time of the first tweet\n",
    "print(\"The time of the first tweet\")\n",
    "print(results_tweets[0].created_at)\n",
    "\n",
    "#Counting retweet counts of the tweets\n",
    "retweet_counts = []\n",
    "for t in results_tweets:\n",
    "    retweet_counts.append(t.retweet_count)\n",
    "    \n",
    "#Plot retweet counts    \n",
    "plt.hist(retweet_counts)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
